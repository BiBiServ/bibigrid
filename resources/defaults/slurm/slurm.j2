# MANAGEMENT POLICIES
SlurmctldHost={{ hostvars[groups.master.0].name | lower }}
AuthType=auth/munge
CryptoType=crypto/munge
SlurmUser=slurm
AuthAltTypes=auth/jwt
AuthAltParameters=jwt_key=/etc/slurm/jwt-secret.key

ClusterName=bibigrid

MpiDefault=none
ProctrackType=proctrack/cgroup # linuxproc # changed for 23.11.0
ReturnToService=2
SwitchType=switch/none
TaskPlugin=task/none
#TaskPlugin=task/cgroup
JobAcctGatherType=jobacct_gather/linux

# SlurmctldParameters=cloud_dns
# didn't work as expected. slurmctld tries to resolve all clients on startup which doesn't work obviously

# PRIORITY
PriorityType=priority/multifactor
PriorityFavorSmall=NO
PriorityWeightJobSize=100000
AccountingStorageTRES=cpu,mem,gres/gpu
PriorityWeightTRES=cpu=1000,mem=2000,gres/gpu=3000

# STATE INFO
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

# CONNECTION
SlurmctldPort=6817
SlurmdPort=6818

# DIRECTORIES
SlurmdSpoolDir=/var/lib/slurm/slurmd
StateSaveLocation=/var/lib/slurm/state_checkpoint

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core

# ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStoreFlags=job_comment
AccountingStorageHost={{ hostvars[groups.master.0].name | lower }}
AccountingStorageUser={{ slurm_conf.db_user }}

# LOGGING
SlurmctldDebug=debug # info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log

# COMPUTE NODES
{% set partitions = {} %}
{% set exclude_groups = [] %}
{% set master_or_empty = groups.master if use_master_as_compute else [] %}
{% set node_groups = [] %}
{% for node_name in master_or_empty + groups.workers %}
{% set node = hostvars[node_name] %}
{% if node.name not in node_groups %}
{% if not node.on_demand %}
{% set _ = exclude_groups.append(node.name) %}
{% endif %}
{% set _ = node_groups.append(node.name) %}
{% set mem = (node.flavor.ram // 1024) * 1000 %}
NodeName={{ node.name }} SocketsPerBoard={{ node.flavor.vcpus }} CoresPerSocket=1 RealMemory={{ mem - [mem // 2, 16000] | min }} State={{ 'CLOUD' if node.on_demand else 'UNKNOWN' }} {{"Features=" + (node.features | join(",")) if node.features is defined }}# {{ node.cloud_identifier }}
{% for partition in node.partitions %}
{% if partition not in partitions %}
{% set _ = partitions.update({partition: []}) %}
{% endif %}
{% set _ = partitions[partition].append(node.name) %}
{% endfor %}
{% endif %}
{% endfor %}

{% for key, value in partitions.items() %}
PartitionName={{ key }} Nodes={{ value | join(",") }}
{% endfor %}

# JobSubmitPlugin
JobSubmitPlugins=all_partitions

# POWER  /ELASTIC SCHEDULING
ResumeProgram=/opt/slurm/create.sh
# Resume time is 15 minutes (900 seconds)
ResumeTimeout= {{ slurm_conf.elastic_scheduling.ResumeTimeout }}
SuspendProgram=/opt/slurm/terminate.sh
# Suspend time is 10 minutes (600 seconds)
SuspendTime= {{ slurm_conf.elastic_scheduling.SuspendTime }}
# Excludes {{ hostvars[groups.master.0].name }} from suspend
SuspendExcNodes={{ exclude_groups | join(',') }}
# Maximum number of nodes
TreeWidth= {{ slurm_conf.elastic_scheduling.TreeWidth }}
# Do not cache dns names
# CommunicationParameters=NoAddrCache # REMOVED for 23.11.0
# Mark node status idle on suspend so DOWN is removed
SlurmctldParameters=idle_on_node_suspend
# Show slurm nodes all the time
PrivateData=cloud
# return node to idle when startup fails
ResumeFailProgram=/opt/slurm/fail.sh

# job container
# TO BE TESTED
JobContainerType=job_container/tmpfs
PrologFlags=Contain
