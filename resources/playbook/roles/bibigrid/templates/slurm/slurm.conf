# MANAGEMENT POLICIES
SlurmctldHost={{ master.name | lower }}
AuthType=auth/munge
CryptoType=crypto/munge
SlurmUser=slurm
AuthAltTypes=auth/jwt
AuthAltParameters=jwt_key=/etc/slurm/jwt-secret.key

ClusterName=bibigrid

MpiDefault=none
ProctrackType=proctrack/linuxproc
ReturnToService=2
SwitchType=switch/none
TaskPlugin=task/none
#TaskPlugin=task/cgroup
JobAcctGatherType=jobacct_gather/linux

# see https://slurm.schedmd.com/slurm.conf.html#OPT_cloud_dns:~:text=for%20additional%20details.-,cloud_dns,-By%20default%2C%20Slurm
# SlurmctldParameters=cloud_dns
# Funktioniert nicht wie vermutet. slurmctld versucht mit diesem Paramter schon beim Start alle Clients aufzulösen,
# was natürlich nicht funktioniert.

# PRIORITY
PriorityType=priority/multifactor
PriorityFavorSmall=NO
PriorityWeightJobSize=100000
AccountingStorageTRES=cpu,mem,gres/gpu
PriorityWeightTRES=cpu=1000,mem=2000,gres/gpu=3000

# STATE INFO
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

# CONNECTION
SlurmctldPort=6817
SlurmdPort=6818

# DIRECTORIES
#JobCheckpointDir=/var/lib/slurm/job_checkpoint
SlurmdSpoolDir=/var/lib/slurm/slurmd
StateSaveLocation=/var/lib/slurm/state_checkpoint

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core

# ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStoreFlags=job_comment
AccountingStorageHost={{ master.name | lower }}
AccountingStorageUser={{ slurm_conf.db_user }}

# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log

# COMPUTE NODES
{% set sl = {"list":[master.name|lower if use_master_as_compute]} %}
{% set mem = master.flavor.ram // 1024 * 1000 %}
NodeName={{ master.name }} SocketsPerBoard={{ master.flavor.vcpus }} CoresPerSocket=1 RealMemory={{ mem - [mem // 2, 16000] | min }} State=UNKNOWN
{% for identifier in workers %}
# {{ identifier }}
{% for type in workers[identifier] %}
{% set mem = type.flavor.ram // 1024 * 1000 %}
NodeName={{ type.name }} SocketsPerBoard={{ type.flavor.vcpus }} CoresPerSocket=1 RealMemory={% if mem < 16001 %}{{ mem - [ mem // 16, 512] | max }}{% endif %}{% if mem > 16000 %}{{ mem - [mem // 16, 4000] | min }}{% endif %} State=CLOUD {{ sl.list.append(type.name)}}
{% endfor %}
PartitionName={{ identifier }} Nodes={{sl.list|join(",")}} default=yes
{{ sl.update({"list":[]}) }}
{% endfor %}


# POWER  /ELASTIC SCHEDULING
ResumeProgram=/opt/slurm/create.sh
# Resume time is 15 minutes (900 seconds)
ResumeTimeout= {{ slurm_conf.elastic_scheduling.ResumeTimeout }}
SuspendProgram=/opt/slurm/terminate.sh
# Suspend time is 10 minutes (600 seconds)
SuspendTime= {{ slurm_conf.elastic_scheduling.SuspendTime }}
# Excludes {{ master.name }} from suspend
SuspendExcNodes={{ master.name }}
# Maximum number of nodes
TreeWidth= {{ slurm_conf.elastic_scheduling.TreeWidth }}
# Do not cache dns names
CommunicationParameters=NoAddrCache
# Mark node status idle on suspend so DOWN is removed
SlurmctldParameters=idle_on_node_suspend
# Show slurm nodes all the time
PrivateData=cloud
# return node to idle when startup fails
ResumeFailProgram=/opt/slurm/fail.sh
