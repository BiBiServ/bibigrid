# MANAGEMENT POLICIES
SlurmctldHost={{ hostvars[groups.master.0].name | lower }}
AuthType=auth/munge
CryptoType=crypto/munge
SlurmUser=slurm
AuthAltTypes=auth/jwt
AuthAltParameters=jwt_key=/etc/slurm/jwt-secret.key

ClusterName=bibigrid

MpiDefault=none
ProctrackType=proctrack/linuxproc
ReturnToService=2
SwitchType=switch/none
TaskPlugin=task/none
#TaskPlugin=task/cgroup
JobAcctGatherType=jobacct_gather/linux

# see https://slurm.schedmd.com/slurm.conf.html#OPT_cloud_dns:~:text=for%20additional%20details.-,cloud_dns,-By%20default%2C%20Slurm
# SlurmctldParameters=cloud_dns
# Funktioniert nicht wie vermutet. slurmctld versucht mit diesem Parameter schon beim Start alle Clients aufzulösen,
# was natürlich nicht funktioniert.

# PRIORITY
PriorityType=priority/multifactor
PriorityFavorSmall=NO
PriorityWeightJobSize=100000
AccountingStorageTRES=cpu,mem,gres/gpu
PriorityWeightTRES=cpu=1000,mem=2000,gres/gpu=3000

# STATE INFO
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

# CONNECTION
SlurmctldPort=6817
SlurmdPort=6818

# DIRECTORIES
#JobCheckpointDir=/var/lib/slurm/job_checkpoint
SlurmdSpoolDir=/var/lib/slurm/slurmd
StateSaveLocation=/var/lib/slurm/state_checkpoint

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core

# ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStoreFlags=job_comment
AccountingStorageHost={{ hostvars[groups.master.0].name | lower }}
AccountingStorageUser={{ slurm_conf.db_user }}

# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log

# COMPUTE NODES
{% set sl = {} %}
{% for node_name in groups.master+groups.workers %}
{% set node = hostvars[node_name] %}
{% set mem = node.flavor.ram // 1024 * 1000 %}
{% if node.cloud_identifier not in sl %}
{{ sl.update({node.cloud_identifier: []}) }}
{% endif %}
{% if node.name not in sl[node.cloud_identifier] %}
NodeName={{ node.name }} SocketsPerBoard={{ node.flavor.vcpus }} CoresPerSocket=1 RealMemory={{ mem - [mem // 2, 16000] | min }} State=CLOUD # {{ node.cloud_identifier }}
{{ sl[node.cloud_identifier].append(node.name)}}
{% endif %}
{% endfor %}
{% for key,value in sl.items() %}
PartitionName={{ key }} Nodes={{value|join(",")}} default=yes
{% endfor %}

# JobSubmitPlugin
JobSubmitPlugins=all_partitions

# POWER  /ELASTIC SCHEDULING
ResumeProgram=/opt/slurm/create.sh
# Resume time is 15 minutes (900 seconds)
ResumeTimeout= {{ slurm_conf.elastic_scheduling.ResumeTimeout }}
SuspendProgram=/opt/slurm/terminate.sh
# Suspend time is 10 minutes (600 seconds)
SuspendTime= {{ slurm_conf.elastic_scheduling.SuspendTime }}
# Excludes {{ hostvars[groups.master.0].name }} from suspend
SuspendExcNodes={{ hostvars[groups.master.0].name }}
# Maximum number of nodes
TreeWidth= {{ slurm_conf.elastic_scheduling.TreeWidth }}
# Do not cache dns names
CommunicationParameters=NoAddrCache
# Mark node status idle on suspend so DOWN is removed
SlurmctldParameters=idle_on_node_suspend
# Show slurm nodes all the time
PrivateData=cloud
# return node to idle when startup fails
ResumeFailProgram=/opt/slurm/fail.sh
